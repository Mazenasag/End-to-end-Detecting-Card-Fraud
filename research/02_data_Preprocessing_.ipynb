{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5aeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e971fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Mlop\\\\End to end\\\\End-to-end-Detecting-Card-Fraud\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fa6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d66d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Mlop\\\\End to end\\\\End-to-end-Detecting-Card-Fraud'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessingConfig:\n",
    "  train_file_path:Path\n",
    "  test_file_path:Path\n",
    "  processed_data_dir: Path\n",
    "  processed_train_file: Path\n",
    "  processed_test_file: Path\n",
    "  remove_duplicates: bool\n",
    "  scaler_type: str\n",
    "  sampling_method: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a681a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CardFraud.constant import*\n",
    "from src.CardFraud.utils.common import read_yaml , create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath=CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(CONFIG_FILE_PATH)\n",
    "        self.params=read_yaml(PARAMS_FILE_PATH)\n",
    "            \n",
    "    def get_data_preprocessing_config(self)-> DataPreprocessingConfig:\n",
    "        data_preprocessing_config =DataPreprocessingConfig(\n",
    "            train_file_path=self.config.data_preprocessing.train_file_path,\n",
    "            test_file_path = self.config.data_preprocessing.test_file_path,\n",
    "            processed_data_dir = self.config.data_preprocessing.processed_data_dir,\n",
    "            processed_train_file = self.config.data_preprocessing.processed_train_file,\n",
    "            processed_test_file =self.config.data_preprocessing.processed_test_file,\n",
    "            remove_duplicates =self.params.d_preprocessing.remove_duplicates,\n",
    "            scaler_type =self.params.d_preprocessing.scaler_type,\n",
    "            sampling_method =self.params.d_preprocessing.sampling_method)\n",
    "        \n",
    "        return data_preprocessing_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea212c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filepath =CONFIG_FILE_PATH\n",
    "config =read_yaml(config_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3662c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts/data_ingestion'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.data_ingestion.root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40880eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from exception import CustomException\n",
    "from logger import logging\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "class Preprocessing:\n",
    "    def __init__(self, config:DataPreprocessingConfig):\n",
    "        self.config=config\n",
    "        \n",
    "    def load_data(self)->pd.DataFrame:\n",
    "        try:\n",
    "            data_train=pd.read_csv(self.config.train_file_path)\n",
    "            data_test=pd.read_csv(self.config.test_file_path)\n",
    "            logging.info(f\"Train Data loaded successfully. Shape: {data_train.shape}\")\n",
    "            logging.info(f\"Test Data loaded successfully. Shape: {data_test.shape}\")\n",
    "      \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "        return data_train,data_test\n",
    "    \n",
    "    def remove_duplicate(self, data:pd.DataFrame):\n",
    "        if self.config.remove_duplicates:\n",
    "            initial_shape=data.shape\n",
    "            unique_data=data.drop_duplicates()\n",
    "            logging.info(f\"Removed duplicates. Shape before: {initial_shape}, after: {data.shape}\")\n",
    "        return unique_data\n",
    "    def apply_oversampling(self,train_data:pd.DataFrame)->pd.DataFrame:\n",
    "        try:\n",
    "            logging.info(\"Applying SMOTE for oversampling...\")\n",
    "            \n",
    "            X_train=train_data.drop(\"Class\" , axis=1)\n",
    "            y_train=train_data[\"Class\"]\n",
    "            smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "            # Recreate DataFrame\n",
    "            oversampled_data = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "            oversampled_data[\"Class\"] = y_resampled\n",
    "            \n",
    "            logging.info(f\"Data after oversampling: {oversampled_data.shape}\")\n",
    "            return oversampled_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during oversampling: {e}\")\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _initialize_scaler(self):\n",
    "        \"\"\"\n",
    "        Initialize the scaler based on the configuration.\n",
    "\n",
    "        Returns:\n",
    "            Scaler object (StandardScaler or MinMaxScaler).\n",
    "        \"\"\"\n",
    "        scaler_type = self.config.scaler_type\n",
    "\n",
    "        if scaler_type == \"StandardScaler\":\n",
    "            return StandardScaler()\n",
    "        elif scaler_type == \"MinMaxScaler\":\n",
    "            return MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scaler type: {scaler_type}\")\n",
    "\n",
    "\n",
    "\n",
    "    def data_scaling(self, train:pd.DataFrame,test:pd.DataFrame):\n",
    "        try:\n",
    "            self.scaler=self._initialize_scaler()\n",
    "            \n",
    "            X_train=train.drop(\"Class\",axis=1)\n",
    "            y_train=train[\"Class\"]\n",
    "            \n",
    "            X_test=test.drop(\"Class\",axis=1)\n",
    "            y_test=test[\"Class\"]\n",
    "            \n",
    "            \n",
    "            X_train_scaled=self.scaler.fit_transform(X_train)\n",
    "            X_test_scaled=self.scaler.transform(X_test)\n",
    "            \n",
    "            X_train_scaled=pd.DataFrame(X_train_scaled,columns=X_train.columns)\n",
    "            X_train_scaled[\"Class\"]=y_train.values\n",
    "            \n",
    "            X_test_scaled=pd.DataFrame(X_test_scaled,columns=X_test.columns)\n",
    "            X_test_scaled[\"Class\"]=y_test.values\n",
    "            logging.info(f\"Data  Scaled sucessfully\")\n",
    "            \n",
    "            return X_train_scaled ,X_test_scaled\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during scaling: {e}\")\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def data_save(self,data:pd.DataFrame,filepath:Path):\n",
    "        os.makedirs(self.config.processed_data_dir,exist_ok=True)\n",
    "        data.to_csv(os.path.join(self.config.processed_data_dir,filepath),index=False)\n",
    "        logging.info(\"data Saved succssfully\")\n",
    "        \n",
    "        \n",
    "    def preprocessing(self):\n",
    "        try:\n",
    "            logging.info(\"Starting preprocessing pipeline...\")\n",
    "            data_train, data_test = self.load_data()\n",
    "\n",
    "            logging.info(\"Removing duplicates...\")\n",
    "            data_train = self.remove_duplicate(data_train)\n",
    "            data_test = self.remove_duplicate(data_test)\n",
    "\n",
    "            logging.info(\"Applying oversampling on training data...\")\n",
    "            data_train = self.apply_oversampling(data_train)\n",
    "\n",
    "            logging.info(\"Scaling data...\")\n",
    "            X_train_scaled, X_test_scaled = self.data_scaling(data_train, data_test)\n",
    "\n",
    "            logging.info(\"Saving preprocessed data...\")\n",
    "            self.data_save(X_train_scaled, Path(self.config.processed_train_file))\n",
    "            self.data_save(X_test_scaled, Path(self.config.processed_test_file))\n",
    "\n",
    "            logging.info(\"Preprocessing pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preprocessing pipeline: {e}\")\n",
    "            raise CustomException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    data_config=ConfigurationManager()\n",
    "    data_processing_config=data_config.get_data_preprocessing_config()\n",
    "    data_processing=Preprocessing(config=data_processing_config)\n",
    "    data_processing.preprocessing()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e21e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
